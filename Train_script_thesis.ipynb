{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/garifollinad/CO2Predict/blob/main/Train_script_thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uqAgktXZ1FCz"
      },
      "outputs": [],
      "source": [
        "# !pip install streamlit\n",
        "# import streamlit as st\n",
        "\n",
        "from sklearn.linear_model import Lasso, LinearRegression\n",
        "# from sklearn import svm\n",
        "# from sklearn.neighbors import KNeighborsRegressor\n",
        "# from sklearn import tree\n",
        "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "# from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.model_selection import TimeSeriesSplit\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from pandas.tseries.offsets import MonthEnd, MonthBegin\n",
        "from datetime import datetime\n",
        "import holidays\n",
        "import time\n",
        "import copy\n",
        "import itertools\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.style.use('ggplot')\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pickle\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F_f4HRBg_LyL"
      },
      "outputs": [],
      "source": [
        "def read_file(file):\n",
        "    employees_data = pd.read_excel(file, sheet_name='Employees')\n",
        "    trips_data = pd.read_excel(file, sheet_name='Trips')\n",
        "    return employees_data, trips_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CRGY403f_ODr"
      },
      "outputs": [],
      "source": [
        "employees_data, trips_data = read_file('/content/FynchData_train.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IiOSCJdb_eCz"
      },
      "outputs": [],
      "source": [
        "def transform_date_columns(trips_data):\n",
        "    dw_mapping={\n",
        "        0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday',\n",
        "        4: 'Friday', 5: 'Saturday', 6: 'Sunday'\n",
        "    }\n",
        "\n",
        "    trips_data['TimeDuration'] = round((trips_data.StopTime - trips_data.StartTime) / np.timedelta64(1, 'h'), 2)\n",
        "    trips_data['Date'] = trips_data.StartTime.dt.date\n",
        "    trips_data['StartTime'] = trips_data.StartTime.dt.time\n",
        "    trips_data['EndTime'] = trips_data.StopTime.dt.time\n",
        "    trips_data.drop(columns='StopTime', inplace=True)\n",
        "    trips_data['Date'] = pd.to_datetime(trips_data['Date'])\n",
        "    trips_data['Day_of_week'] = trips_data.Date.dt.weekday.map(dw_mapping)\n",
        "    return trips_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DtFI6aYA_qwB"
      },
      "outputs": [],
      "source": [
        "trips_data = transform_date_columns(trips_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5lkxDmmf_ud4"
      },
      "outputs": [],
      "source": [
        "def get_stat_by_group(group_column, stat_columns, df):\n",
        "    group_names = df[group_column].unique()\n",
        "    info = pd.DataFrame(columns=['group', 'Q1', 'mean', 'median', 'Q3', 'IQR', 'count', 'min_val', 'max_val'])\n",
        "    for g in group_names:\n",
        "        data = df.loc[df[group_column] == g]\n",
        "        q1 = data[stat_columns].quantile(0.25)\n",
        "        mean = data[stat_columns].mean()\n",
        "        median = data[stat_columns].quantile(0.5)\n",
        "        q3 = data[stat_columns].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        min_val = q1 - 1.5 * iqr\n",
        "        max_val = q3 + 1.5 * iqr\n",
        "        info.loc[len(info)] = [g, q1, mean, median, q3, iqr, len(data), min_val, max_val]\n",
        "    return info\n",
        "\n",
        "def delete_outliers(commute_trips):\n",
        "    commute_trips = commute_trips.loc[commute_trips['Co2_emissions'] > 0]\n",
        "    commute_trips = commute_trips.loc[commute_trips['TimeDuration'] < 4]\n",
        "    commute_trips = commute_trips[~commute_trips.Day_of_week.isin(['Saturday', 'Sunday'])]\n",
        "\n",
        "    commute_trips.loc[(commute_trips['FuelType'].notnull()) &\n",
        "               (commute_trips['Vehicle'].isin(['Bike', 'Foot', 'SharedBike'])), 'FuelType'] = np.NaN\n",
        "    commute_trips['VehicleWithFuel'] = np.where(\n",
        "        (~commute_trips['FuelType'].isnull()) & (commute_trips['Vehicle'].str.contains('Car')),\n",
        "        commute_trips['Vehicle'][(~commute_trips['FuelType'].isnull()) & (commute_trips['Vehicle'].str.contains('Car'))] + ' ' + commute_trips['FuelType'],\n",
        "        commute_trips['Vehicle'])\n",
        "    commute_trips = commute_trips.loc[~commute_trips['Vehicle'].isin(['Plane', 'Boat', 'SharedElectricCar', 'Motorbike'])]\n",
        "    commute_trips['Vehicle'] = np.where((commute_trips['FuelType'] == 'electrisch') & (commute_trips['Vehicle'].str.contains('Car')), 'ElectricCar', commute_trips['Vehicle'])\n",
        "\n",
        "    commute_trips['Emission by km'] = round(commute_trips['Co2_emissions'] / commute_trips['Distance'], 2)\n",
        "    vehicle_emissions_km_info = get_stat_by_group('Vehicle', 'Emission by km', commute_trips)\n",
        "    outliers_by_emission_km = pd.DataFrame(columns=commute_trips.columns)\n",
        "    for i in range(len(vehicle_emissions_km_info)):\n",
        "        data = commute_trips.loc[(commute_trips['Vehicle'] == vehicle_emissions_km_info.iloc[i][0]) &\n",
        "                                 ((commute_trips['Emission by km'] < vehicle_emissions_km_info.iloc[i][7]) | (commute_trips['Emission by km'] > vehicle_emissions_km_info.iloc[i][8]))]\n",
        "        outliers_by_emission_km = outliers_by_emission_km.append(data)\n",
        "    commute_trips = commute_trips.drop(outliers_by_emission_km.index)\n",
        "\n",
        "    commute_trips['Avg Velocity'] = commute_trips['Distance'] // commute_trips['TimeDuration']\n",
        "    max_avg_velocity= {'Car': 120, 'Bike': 25, 'Foot': 15, 'PublicTransport': 70,\n",
        "                       'Carpool': 120, 'ElectricBike': 25, 'Train': 150,\n",
        "                       'ElectricCar': 120, 'SharedBike': 25, 'Moped': 50}\n",
        "    vehicle_velocity_info = get_stat_by_group('Vehicle', 'Avg Velocity', commute_trips)\n",
        "    outliers_by_velocity = pd.DataFrame(columns=commute_trips.columns)\n",
        "    for i in range(len(vehicle_velocity_info)):\n",
        "        human_max_velocity = max_avg_velocity[vehicle_velocity_info.iloc[i][0]]\n",
        "        min_val, max_val = vehicle_velocity_info.iloc[i][7], max(human_max_velocity,  vehicle_velocity_info.iloc[i][8])\n",
        "        data = commute_trips.loc[(commute_trips['Vehicle'] == vehicle_velocity_info.iloc[i][0]) &\n",
        "                                 ((commute_trips['Avg Velocity'] < min_val) | (commute_trips['Avg Velocity'] > max_val))]\n",
        "        outliers_by_velocity = outliers_by_velocity.append(data)\n",
        "    commute_trips = commute_trips.drop(outliers_by_velocity.index)\n",
        "    return commute_trips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aOysN9ag_9p3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c827a20e-ab48-46d4-8340-cccab38c78e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['EmployeeId', 'TripId', 'TripType', 'Vehicle', 'Date', 'Day_of_week',\n",
            "       'StartTime', 'EndTime', 'TimeDuration', 'Distance', 'Co2_emissions',\n",
            "       'CompanyId', 'FuelType', 'VehicleWithFuel', 'Emission by km',\n",
            "       'Avg Velocity'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "trips_data = trips_data[['EmployeeId', 'TripId', 'TripType', 'Vehicle', 'Date', 'Day_of_week', 'StartTime', 'EndTime', 'TimeDuration', 'Distance', 'Co2_emissions']]\n",
        "trips_data = pd.merge(trips_data, employees_data[['EmployeeId', 'CompanyId', 'FuelType']], on='EmployeeId', how='left')\n",
        "commute_trips = trips_data.loc[trips_data['TripType'] == 'commute']\n",
        "updated_commute_trips = delete_outliers(commute_trips)\n",
        "print(updated_commute_trips.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Fe4QBfr2AM42"
      },
      "outputs": [],
      "source": [
        "def lag_feature(df_grouped, shift_months, cols):\n",
        "  for c in cols:\n",
        "    tmp = df_grouped[['Date', 'CompanyId', 'VehicleWithFuel', c]]\n",
        "    for i in range(1, shift_months+1):\n",
        "      shifted = tmp.copy()\n",
        "      shifted.columns = ['Date', 'CompanyId', 'VehicleWithFuel', c + \"_lag_\"+str(i)]\n",
        "      shifted.Date = shifted.Date + pd.DateOffset(months=i)\n",
        "      df_grouped = pd.merge(df_grouped, shifted, on=['Date', 'CompanyId', 'VehicleWithFuel'], how='left')\n",
        "  return df_grouped\n",
        "\n",
        "def count_workdays(df):\n",
        "    h = holidays.Netherlands()\n",
        "    b = pd.bdate_range(df['Date'], df['Date'] + MonthEnd(0))\n",
        "    return sum(y not in h for y in b)\n",
        "\n",
        "def train_valid_split(df):\n",
        "  months = df.index.unique()\n",
        "  n_months = len(months)\n",
        "  # print(months)\n",
        "  if len(months) < 2:\n",
        "    raise ValueError(\"At least should be two months data\")\n",
        "\n",
        "  for m in range(1, n_months-1):\n",
        "    test_idx = months[m]\n",
        "    train_idx = months[:m]\n",
        "\n",
        "    yield train_idx, test_idx\n",
        "\n",
        "def create_models(type, params, train_df_len=0):\n",
        "  new_params = copy.deepcopy(params)\n",
        "  models = [None] * len(new_params)\n",
        "  for i in range(len(new_params)):\n",
        "    if type == 'HistGradientBoostingRegressor':\n",
        "      new_val = max(1, round(new_params[i]['min_samples_leaf'] * train_df_len))\n",
        "      new_params[i]['min_samples_leaf'] = new_val\n",
        "\n",
        "      reg = HistGradientBoostingRegressor()\n",
        "    else: # type == 'RandomForestRegressor'\n",
        "      reg = RandomForestRegressor()\n",
        "\n",
        "    reg.set_params(**new_params[i])\n",
        "    models[i] = reg\n",
        "  return models\n",
        "    # print(valid_vals)\n",
        "\n",
        "# def scale_needed_columns(scaler, X, cols, only_transform=True):\n",
        "#   other_df = X[~X.columns.isin(cols)]\n",
        "#   transform_df = X[X.columns.isin(cols)]\n",
        "\n",
        "#   if only_transform:\n",
        "#     transformed_df = scaler.transform(transform_df)\n",
        "#   else:\n",
        "#     transformed_df = scaler.fit_transform(transform_df)\n",
        "\n",
        "#   output_df = pd.concat([transformed_df, other_df], axis=1)\n",
        "#   return output_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "edd48aRQAgNa"
      },
      "outputs": [],
      "source": [
        "def train_model(updated_commute_trips):\n",
        "  model_data = updated_commute_trips.copy()\n",
        "  model_data.drop(columns=['EmployeeId', 'TripId', 'TripType', 'Vehicle', 'FuelType', 'Day_of_week', 'EndTime', 'Emission by km', 'Avg Velocity'], inplace=True)\n",
        "  model_data['StartTime'] = model_data['StartTime'].apply(lambda x: round(x.hour + x.minute/60, 2))\n",
        "\n",
        "  model_data['Date'] = model_data['Date'] + MonthEnd(0) - MonthBegin(1)\n",
        "\n",
        "  model_data = model_data.loc[model_data['Date'] > '2022-04-01', :]\n",
        "\n",
        "  model_data_monthly = model_data.copy()\n",
        "  model_data_monthly.sort_values(['VehicleWithFuel', 'CompanyId', 'Date'], inplace=True)\n",
        "\n",
        "  model_data_monthly = model_data_monthly.groupby(['Date', 'CompanyId', 'VehicleWithFuel'], as_index=False)\n",
        "\n",
        "  model_data_monthly = model_data_monthly.agg({'TimeDuration': ['mean'], 'Distance': ['mean'],\n",
        "                          'Co2_emissions': ['mean', 'sum', 'count']})\n",
        "  model_data_monthly.columns = ['Date', 'CompanyId', 'VehicleWithFuel',\n",
        "                                'TimeDuration_mean', 'Distance_mean', 'Co2_mean', 'Co2_sum', 'Count']\n",
        "\n",
        "  model_data_monthly_with_lag = lag_feature(model_data_monthly, 3,\n",
        "      ['TimeDuration_mean', 'Distance_mean', 'Co2_mean', 'Count'])\n",
        "\n",
        "  model_data_monthly_with_lag.fillna(0, inplace=True)\n",
        "  model_data_monthly_with_lag['qmean_cnt'] = model_data_monthly_with_lag[['Count_lag_1',\n",
        "                                  'Count_lag_2',\n",
        "                                  'Count_lag_3']].mean(skipna=True, axis=1)\n",
        "  # Add quater std count\n",
        "  model_data_monthly_with_lag['qstd_cnt'] = model_data_monthly_with_lag[['Count_lag_1',\n",
        "                                      'Count_lag_2',\n",
        "                                      'Count_lag_3']].std(skipna=True, axis=1)\n",
        "  # Add quater min count\n",
        "  model_data_monthly_with_lag['qmin_cnt'] = model_data_monthly_with_lag[['Count_lag_1',\n",
        "                                      'Count_lag_2',\n",
        "                                      'Count_lag_3']].min(skipna=True, axis=1)\n",
        "  # Add quater max count\n",
        "  model_data_monthly_with_lag['qmax_cnt'] = model_data_monthly_with_lag[['Count_lag_1',\n",
        "                                      'Count_lag_2',\n",
        "                                      'Count_lag_3']].max(skipna=True, axis=1)\n",
        "\n",
        "  ohe = OneHotEncoder()\n",
        "  # le = LabelEncoder()\n",
        "  ohe_encode_columns = ['VehicleWithFuel']\n",
        "  # le_encode_columns = ['CompanyId']\n",
        "  ohe_encoded_array = ohe.fit_transform(model_data_monthly_with_lag[ohe_encode_columns]).toarray()\n",
        "  ohe_encoded_labels = ohe.get_feature_names_out()\n",
        "  ohe_encoded_df = pd.DataFrame(ohe_encoded_array, columns=ohe_encoded_labels)\n",
        "  model_data_monthly_with_lag.drop(columns=ohe_encode_columns, inplace=True)\n",
        "  model_data_monthly_with_lag = pd.concat([model_data_monthly_with_lag, ohe_encoded_df], axis=1)\n",
        "  model_data_monthly_with_lag.dropna(inplace=True)\n",
        "  # model_data_monthly_with_lag['CompanyId'] = le.fit_transform(model_data_monthly_with_lag[le_encode_columns])\n",
        "\n",
        "  nl_holidays = holidays.Netherlands()\n",
        "  months = model_data_monthly_with_lag.index.unique()\n",
        "  model_data_monthly_with_lag['Nb_work_days'] =  model_data_monthly_with_lag.apply(count_workdays, axis=1)\n",
        "\n",
        "  model_data_monthly_with_lag['Year'] = model_data_monthly_with_lag['Date'].dt.year\n",
        "  model_data_monthly_with_lag['Month'] = model_data_monthly_with_lag['Date'].dt.month\n",
        "  model_data_monthly_with_lag['Week'] = model_data_monthly_with_lag['Date'].dt.week\n",
        "\n",
        "\n",
        "  # For count predict model\n",
        "  count_columns = ['Year', 'Month', 'Week',\n",
        "      'Count_lag_1', 'Count_lag_2', 'Count_lag_3',\n",
        "      'qmean_cnt', 'qstd_cnt', 'qmin_cnt', 'qmax_cnt', 'Nb_work_days']\n",
        "  # scale_count_columns = count_columns.copy()\n",
        "  ohe_encoded_labels_list = ohe_encoded_labels.tolist()\n",
        "  count_columns.extend(ohe_encoded_labels_list)\n",
        "\n",
        "  # For co2_mean predict model\n",
        "  co2_columns = ['Year', 'Month', 'Week',\n",
        "      'TimeDuration_mean_lag_1', 'TimeDuration_mean_lag_2','TimeDuration_mean_lag_3',\n",
        "      'Distance_mean_lag_1', 'Distance_mean_lag_2', 'Distance_mean_lag_3',\n",
        "      'Co2_mean_lag_1', 'Co2_mean_lag_2','Co2_mean_lag_3']\n",
        "  # scale_co2_columns = co2_columns.copy()\n",
        "  co2_columns.extend(ohe_encoded_labels_list)\n",
        "\n",
        "  vehicle_columns_f = 'vehicle_columns.sav'\n",
        "  pickle.dump(ohe_encoded_labels_list, open(vehicle_columns_f, 'wb'))\n",
        "\n",
        "  model_data_monthly_with_lag.set_index('Date', inplace=True)\n",
        "\n",
        "  r1 = LinearRegression()\n",
        "  r1_models = [r1]\n",
        "  # results1 = {}\n",
        "\n",
        "  # r2 = HistGradientBoostingRegressor()\n",
        "  param2 = {\n",
        "      'loss': ['squared_error'],\n",
        "      'learning_rate': [0.5, 1],\n",
        "      'max_iter': [1, 4, 16],\n",
        "      'max_depth': [2, 4],\n",
        "      'min_samples_leaf': [0.2, 0.6],\n",
        "      'l2_regularization': [0, 0.1, 0.01]\n",
        "  }\n",
        "  keys, values = zip(*param2.items())\n",
        "  r2_model_params = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "  # r2_models = create_models('HistGradientBoostingRegressor', r2_model_params)\n",
        "  # results2 = {}\n",
        "\n",
        "  # r3 = RandomForestRegressor()\n",
        "  param3 = {\n",
        "      'n_estimators': [100, 200, 300, 400],\n",
        "      'bootstrap': [True, False],\n",
        "      'max_depth': [5, 10, 20],\n",
        "      'min_samples_leaf': [4, 8, 16],\n",
        "      'min_samples_split': [5, 10]\n",
        "  }\n",
        "  keys, values = zip(*param3.items())\n",
        "  r3_model_params = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "  r3_models = create_models('RandomForestRegressor', r3_model_params)\n",
        "\n",
        "  model_res = pd.DataFrame(columns=['Model', 'Model_index', 'Target', 'CompanyId', 'Iter', 'RMSE', 'MAE', 'r2', 'Adj_r2', 'Time'])\n",
        "\n",
        "  for c in model_data_monthly_with_lag['CompanyId'].unique():\n",
        "    data = model_data_monthly_with_lag.loc[model_data_monthly_with_lag['CompanyId'] == c, :]\n",
        "\n",
        "    y_train_valid = data['Co2_sum']\n",
        "    y_count = data['Count']\n",
        "    X_count = data.loc[:, count_columns]\n",
        "    y_co2 = data['Co2_mean']\n",
        "    X_co2 = data.loc[:, co2_columns]\n",
        "\n",
        "    for iter, (train_index, valid_index) in enumerate(train_valid_split(data)):\n",
        "      print(iter)\n",
        "      X_train_count, X_valid_count = X_count.loc[train_index, :], X_count.loc[valid_index,:]\n",
        "      # X_train_count_scaled, X_valid_count_scaled = X_count_scaled.iloc[train_index, :], X_count_scaled.iloc[test_index,:]\n",
        "      y_train_count, y_valid_count = y_count.loc[train_index], y_count.loc[valid_index]\n",
        "      X_train_co2, X_valid_co2 = X_co2.loc[train_index, :], X_co2.loc[valid_index,:]\n",
        "      # X_train_co2_scaled, X_valid_co2_scaled = X_co2_scaled.iloc[train_index, :], X_co2_scaled.iloc[test_index,:]\n",
        "      y_train_co2, y_valid_co2 = y_co2.loc[train_index], y_co2.loc[valid_index]\n",
        "      y_train, y_valid = y_train_valid.loc[train_index], y_train_valid.loc[valid_index]\n",
        "\n",
        "      r2_models = create_models('HistGradientBoostingRegressor', r2_model_params, X_train_count.shape[0])\n",
        "\n",
        "      for i, r1 in enumerate(r1_models):\n",
        "        print('LinearRegression', i)\n",
        "        t0 = time.time()\n",
        "        r1.fit(X_train_count, y_train_count)\n",
        "        t = time.time()-t0\n",
        "        y_predict = r1.predict(X_valid_count)\n",
        "        rmse = mean_squared_error(y_valid_count, y_predict, squared=False)\n",
        "        mae = mean_absolute_error(y_valid_count, y_predict)\n",
        "        r2_s = r2_score(y_valid_count, y_predict)\n",
        "        n, p = X_valid_count.shape[0], X_valid_count.shape[1]\n",
        "        adj_r2 = 1 - (1 - r2_s) * ((n - 1)/(n - p - 1))\n",
        "        model_res.loc[len(model_res)] = ['LinearRegression', i, 'Count', c, iter, rmse, mae, r2_s, adj_r2, t]\n",
        "\n",
        "        t0 = time.time()\n",
        "        r1.fit(X_train_co2, y_train_co2)\n",
        "        t = time.time()-t0\n",
        "        y_predict = r1.predict(X_valid_co2)\n",
        "        rmse = mean_squared_error(y_valid_co2, y_predict, squared=False)\n",
        "        mae = mean_absolute_error(y_valid_co2, y_predict)\n",
        "        r2_s = r2_score(y_valid_co2, y_predict)\n",
        "        n, p = X_valid_co2.shape[0], X_valid_co2.shape[1]\n",
        "        adj_r2 = 1 - (1 - r2_s) * ((n - 1)/(n - p - 1))\n",
        "        model_res.loc[len(model_res)] = ['LinearRegression', i, 'Co2_mean', c, iter, rmse, mae, r2_s, adj_r2, t]\n",
        "\n",
        "      for i, r2 in enumerate(r2_models):\n",
        "        print('HistGradientBoostingRegressor', i)\n",
        "        t0 = time.time()\n",
        "        r2.fit(X_train_count, y_train_count)\n",
        "        t = time.time()-t0\n",
        "        y_predict = r2.predict(X_valid_count)\n",
        "        rmse = mean_squared_error(y_valid_count, y_predict, squared=False)\n",
        "        mae = mean_absolute_error(y_valid_count, y_predict)\n",
        "        r2_s = r2_score(y_valid_count, y_predict)\n",
        "        n, p = X_valid_count.shape[0], X_valid_count.shape[1]\n",
        "        adj_r2 = 1 - (1 - r2_s) * ((n - 1)/(n - p - 1))\n",
        "        model_res.loc[len(model_res)] = ['HistGradientBoostingRegressor', i, 'Count', c, iter, rmse, mae, r2_s, adj_r2, t]\n",
        "\n",
        "        t0 = time.time()\n",
        "        r2.fit(X_train_co2, y_train_co2)\n",
        "        t = time.time()-t0\n",
        "        y_predict = r2.predict(X_valid_co2)\n",
        "        rmse = mean_squared_error(y_valid_co2, y_predict, squared=False)\n",
        "        mae = mean_absolute_error(y_valid_co2, y_predict)\n",
        "        r2_s = r2_score(y_valid_co2, y_predict)\n",
        "        n, p = X_valid_co2.shape[0], X_valid_co2.shape[1]\n",
        "        adj_r2 = 1 - (1 - r2_s) * ((n - 1)/(n - p - 1))\n",
        "        model_res.loc[len(model_res)] = ['HistGradientBoostingRegressor', i, 'Co2_mean', c, iter, rmse, mae, r2_s, adj_r2, t]\n",
        "\n",
        "      for i, r3 in enumerate(r3_models):\n",
        "        print('RandomForestRegressor', i)\n",
        "        t0 = time.time()\n",
        "        r3.fit(X_train_count, y_train_count)\n",
        "        t = time.time()-t0\n",
        "        y_predict = r3.predict(X_valid_count)\n",
        "        rmse = mean_squared_error(y_valid_count, y_predict, squared=False)\n",
        "        mae = mean_absolute_error(y_valid_count, y_predict)\n",
        "        r2_s = r2_score(y_valid_count, y_predict)\n",
        "        n, p = X_valid_count.shape[0], X_valid_count.shape[1]\n",
        "        adj_r2 = 1 - (1 - r2_s) * ((n - 1)/(n - p - 1))\n",
        "        model_res.loc[len(model_res)] = ['RandomForestRegressor', i, 'Count', c, iter, rmse, mae, r2_s, adj_r2, t]\n",
        "\n",
        "        t0 = time.time()\n",
        "        r3.fit(X_train_co2, y_train_co2)\n",
        "        t = time.time()-t0\n",
        "        y_predict = r3.predict(X_valid_co2)\n",
        "        rmse = mean_squared_error(y_valid_co2, y_predict, squared=False)\n",
        "        mae = mean_absolute_error(y_valid_co2, y_predict)\n",
        "        r2_s = r2_score(y_valid_co2, y_predict)\n",
        "        n, p = X_valid_co2.shape[0], X_valid_co2.shape[1]\n",
        "        adj_r2 = 1 - (1 - r2_s) * ((n - 1)/(n - p - 1))\n",
        "        model_res.loc[len(model_res)] = ['RandomForestRegressor', i, 'Co2_mean', c, iter, rmse, mae, r2_s, adj_r2, t]\n",
        "\n",
        "  results_copy = model_res.copy()\n",
        "  scaled_results = pd.DataFrame(columns = ['Model', 'Model_index', 'Target', 'CompanyId', 'Iter', 'RMSE', 'MAE', 'r2', 'Adj_r2', 'Time'])\n",
        "  cols = scaled_results.columns.tolist()\n",
        "\n",
        "  scaler = Pipeline(steps=[\n",
        "          ('standard', RobustScaler())])\n",
        "\n",
        "  for t in results_copy['Target'].unique():\n",
        "    for c in results_copy['CompanyId'].unique():\n",
        "      scale_data = results_copy.loc[(results_copy['CompanyId'] == c) & (results_copy['Target'] == t), :]\n",
        "      ct = ColumnTransformer(transformers=[('std', scaler , ['RMSE', 'MAE'])],\n",
        "                          remainder='passthrough')\n",
        "      res = pd.DataFrame(ct.fit_transform(scale_data),\n",
        "                        columns = ['RMSE', 'MAE', 'Model', 'Model_index', 'Target', 'CompanyId', 'Iter', 'r2', 'Adj_r2', 'Time'])\n",
        "      res = res[cols[:2] + cols[2:]]\n",
        "      scaled_results = pd.concat([scaled_results, res], axis=0)\n",
        "\n",
        "  scaled_results.rename(columns={'RMSE': 'RMSE_sc', 'MAE': 'MAE_sc'}, inplace=True)\n",
        "\n",
        "  scaled_results.drop(['CompanyId', 'Iter'], axis=1, inplace=True)\n",
        "  final_res = scaled_results.groupby(['Model', 'Model_index', 'Target'], as_index=False).mean()\n",
        "\n",
        "  final_res['Error'] = 0.7 * final_res['RMSE_sc'] + 0.3 * final_res['MAE_sc']\n",
        "  for t in final_res['Target'].unique():\n",
        "    top_model = final_res.loc[final_res['Target'] == t, :].sort_values('Error').head(1)\n",
        "    model_type, model_idx = top_model['Model'].values[0], top_model['Model_index'].values[0]\n",
        "\n",
        "    if model_type == 'HistGradientBoostingRegressor':\n",
        "      model = HistGradientBoostingRegressor()\n",
        "      model.set_params(**r2_model_params[model_idx])\n",
        "    elif model_type == 'RandomForestRegressor':\n",
        "      model = RandomForestRegressor()\n",
        "      model.set_params(**r3_model_params[model_idx])\n",
        "    else:\n",
        "      model = LinearRegression()\n",
        "\n",
        "    model_f = f'model_{t}.sav'\n",
        "    pickle.dump(model, open(model_f, 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xATJlGZglIC2"
      },
      "outputs": [],
      "source": [
        "train_model(updated_commute_trips)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNe7jHLMiRwPkkgjl/Q8VCW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}